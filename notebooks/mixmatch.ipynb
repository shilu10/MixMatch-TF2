{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import time, os\nimport numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt \nimport seaborn as ans \nfrom tqdm import tqdm \nimport shutil \nimport tensorflow as tf \nfrom tensorflow import keras \nfrom tensorflow.keras import Model\nfrom tensorflow.keras import layers \nfrom tensorflow.keras.layers import *\nimport tensorflow_datasets as tfds\nfrom tensorflow.keras.initializers import RandomNormal\nfrom tensorflow.keras import *\nfrom datetime import datetime\nfrom tensorflow.keras.applications.vgg19 import VGG19\nfrom tensorflow.keras.activations import sigmoid\nfrom tensorflow.keras.layers import Dense, Input, UpSampling2D, Conv2DTranspose, Conv2D, add, Add,\\\n                    Lambda, Concatenate, AveragePooling2D, BatchNormalization, GlobalAveragePooling2D, \\\n                    Add, LayerNormalization, Activation, LeakyReLU, SeparableConv2D, Softmax, MaxPooling2D","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class DataLoader: \n    \"\"\"\n        Class, will be useful for creating the BYOL dataset or dataset for the DownStream task \n            like classification or segmentation.\n        Methods:\n            __download_data(scope: private)\n            __normalize(scope: private)\n            __preprocess_img(scope: private)\n             __get_valdata(scope: private)\n            get_dataset(scope: public)\n            __create_tf_dataset(scope: public)\n        \n        Property:\n            dname(dtype: str)        : dataset name(supports cifar10, cifar100).\n            n_val(type: int)         : Number of validation data needed, this will be created by splitting the testing\n                                       data.\n            resize_shape(dtype: int) : Resize shape, bcoz pretrained models, might have a different required shape.\n            normalize(dtype: bool)   : bool value, whether to normalize the data or not. \n            n_labeled(dtype: int)    : number of training samples needed to be labeled.\n    \"\"\"\n    \n    def __init__(self, dname=\"cifar10\", n_val=5000, normalize=True, n_labelled_samples=100): \n        assert dname in [\"cifar10\", 'cifar100', \"svhn\"], \"supported datasets are cifar10, cifar100,svhn\"\n        assert n_val <= 10_000, \"ValueError: nval value should be <= 10_000\"\n        \n        self.__n_labelled_samples = n_labelled_samples\n        train_data, test_data = self.__download_data(dname)\n        self.__train_X, self.__train_y = train_data\n        self.__dtest_X, self.__dtest_y = test_data \n        \n        self.__get_unlabeled_data()\n        self.__get_valdata(n_val)        \n        self.__normalize() if normalize else None\n        \n    def __len__(self): \n        return self.__train_X.shape[0] + self.__dtest_X.shape[0]\n    \n    def __repr__(self): \n        return f\"Training Samples: {self.__train_X.shape[0]}, Testing Samples: {self.__dtest_X.shape[0]}\"\n    \n    def __download_data(self, dname):\n        \"\"\"\n            Downloads the data from the tensorflow website using the tensorflw.keras.load_data() method.\n            Params:\n                dname(type: Str): dataset name, it just supports two dataset cifar10 or cifar100\n            Return(type(np.ndarray, np.ndarray))\n                returns the training data and testing data\n        \"\"\"\n        if dname == \"cifar10\": \n            train_data, test_data = tf.keras.datasets.cifar10.load_data()\n            self.__n_labels = len(np.unique(test_data[1]))\n            \n        if dname == \"cifar100\": \n            train_data, test_data = tf.keras.datasets.cifar100.load_data()\n            self.__n_labels = len(np.unique(test_data[1]))\n            \n        if dname == \"svhn\":\n            dataset = tfds.load(name='svhn_cropped')\n            train_data = dataset['train']\n            test_data = dataset['test']\n            self.__n_labels = len(np.unique(test_data[1]))\n            \n        return train_data, test_data\n    \n    def __normalize(self): \n        \"\"\"\n            this method, will used to normalize the inputs.\n        \"\"\"\n        self.__train_X = self.__train_X / 255.0\n        self.__dtest_X = self.__dtest_X / 255.0\n    \n    def __val_test_preprocess(self, x, label):\n        x = tf.image.convert_image_dtype(x, dtype=tf.float32)\n        label = tf.one_hot(label, self.__n_labels)\n        label = tf.reshape(label, (x.shape[0], self.__n_labels))\n        return x, label\n\n    def __preprocess_tf_dataset(self, tf_ds, batch_size, transform=False, subset=\"unlablled\"):\n        try:\n            tf_ds = tf_ds.shuffle(1024, seed=42)\n            tf_ds = tf_ds.batch(batch_size, drop_remainder=True)\n            if transform:\n                if subset == 'unlabelled':\n                    tf_ds = tf_ds.map(lambda x: self.__augment(x, is_label=False),\n                                          num_parallel_calls=tf.data.experimental.AUTOTUNE)\n                    \n                else:\n                    tf_ds = tf_ds.map(lambda x, y: self.__augment(x, y),\n                                          num_parallel_calls=tf.data.experimental.AUTOTUNE)\n            \n            if subset in [\"test\", \"val\"]:\n                print(\"in\")\n                tf_ds = tf_ds.map(lambda x, y: self.__val_test_preprocess(x, y),\n                                          num_parallel_calls=tf.data.experimental.AUTOTUNE)\n                \n            \n            tf_ds = tf_ds.prefetch(tf.data.experimental.AUTOTUNE)   \n            return tf_ds\n        \n        except Exception as err:\n            return err\n    \n    def get_dataset(self, batch_size, subset=\"unlabelled\",\n                                        transform=False, k_augmentation=1):\n        \"\"\"\n            this method, will gives the byol dataset, which is nothing\n            but a tf.data.Dataset object.\n            Params:\n                batch_size(dtype: int)   : Batch Size.\n                subset(dtype: str) : which type of dataset needed\n                \n            return(type: tf.data.Dataset)\n                returns the tf.data.Dataset for intended dataset_type,\n                by preprocessing and converting the np data.\n        \"\"\"\n        try:\n            if subset == \"unlabelled\":\n                tf_ds = tf.data.Dataset.from_tensor_slices((self.__unlabelled_X))\n                res = []\n                \n                for _ in range(k_augmentation):\n                    inner_res = self.__preprocess_tf_dataset(\n                                            tf_ds=tf_ds,\n                                            batch_size=batch_size,\n                                            transform=transform,\n                                            subset=subset\n                                        )\n                    res.append(inner_res)\n                \n                return tf.data.Dataset.zip(tuple(res))\n            \n            if subset == \"labelled\":\n                tf_ds = tf.data.Dataset.from_tensor_slices((self.__labelled_X, self.__labelled_y))\n                tf_ds = self.__preprocess_tf_dataset(\n                                        tf_ds=tf_ds,\n                                        batch_size=batch_size,\n                                        transform=transform,\n                                        subset=subset\n                                    )\n                return tf_ds  \n            \n            if subset == \"val\":\n                tf_ds = tf.data.Dataset.from_tensor_slices((self.__val_X, self.__val_y))\n                tf_ds = self.__preprocess_tf_dataset(\n                                        tf_ds=tf_ds,\n                                        batch_size=batch_size,\n                                        transform=transform,\n                                        subset=subset\n                                    )\n                return tf_ds  \n            \n            if subset == \"test\":\n                tf_ds = tf.data.Dataset.from_tensor_slices((self.__test_X, self.__test_y))\n                tf_ds = self.__preprocess_tf_dataset(\n                                        tf_ds=tf_ds,\n                                        batch_size=batch_size,\n                                        transform=transform,\n                                        subset=subset\n                                    )\n                return tf_ds  \n        \n        except Exception as err:\n            return err\n    \n    def __get_valdata(self, nval):\n        \"\"\"\n            this method is used to create a validation data by randomly sampling from the testing data.\n            Params:\n                nval(dtype: Int); Number of validation data needed, rest of test_X.shape[0] - nval, will be \n                                  testing data size.\n            returns(type; np.ndarray, np.ndarray):\n                returns the testing and validation dataset.\n        \"\"\"\n        try: \n            ind_arr = np.arange(10_000)\n            val_inds = np.random.choice(ind_arr, nval, replace=False)\n            test_inds = [i for i in ind_arr if not i in val_inds]\n\n            self.__test_X, self.__test_y = self.__dtest_X[test_inds], self.__dtest_y[test_inds]\n            self.__val_X, self.__val_y = self.__dtest_X[val_inds], self.__dtest_y[val_inds]\n            \n        except Exception as err:\n            raise err    \n            \n    def __get_unlabeled_data(self):\n        try:\n            ind_arr = np.arange(40_000)\n            labelled_inds = np.random.choice(\n                                            ind_arr,\n                                            self.__n_labelled_samples,\n                                            replace=False\n                                        )\n            unlabelled_inds = [i for i in ind_arr if not i in labelled_inds]\n            self.__labelled_X = self.__train_X[labelled_inds]\n            self.__labelled_y = self.__train_y[labelled_inds]\n\n            self.__unlabelled_X = self.__train_X[unlabelled_inds]\n            self.__unlabelled_y = self.__train_y[unlabelled_inds]\n        \n        except Exception as err:\n            return err \n    \n    @tf.function\n    def __augment(self, x, label=None, is_label=True):\n        try:\n            x = tf.image.convert_image_dtype(x, dtype=tf.float32)\n            # random left right flipping\n            x = tf.image.random_flip_left_right(x)\n            # random pad and crop\n            x = tf.pad(x, paddings=[(0, 0), (4, 4), (4, 4), (0, 0)], mode='REFLECT')\n            x = tf.map_fn(lambda batch: tf.image.random_crop(batch, size=(32, 32, 3)), x)\n            if not is_label:\n                return x\n            else:\n                label = tf.one_hot(label, self.__n_labels)\n                label = tf.reshape(label, (x.shape[0], self.__n_labels))\n                return x, label\n            \n        except Exception as err:\n            return err\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mixmatch_dataloader = DataLoader()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"unlabelled_ds = mixmatch_dataloader.get_dataset(32, \"unlabelled\", True, 2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labelled_ds = mixmatch_dataloader.get_dataset(32, \"labelled\", True, 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_ds = mixmatch_dataloader.get_dataset(32, \"val\", False, 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_ds","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\n\nclass Residual3x3Unit(tf.keras.layers.Layer):\n    def __init__(self, channels_in, channels_out, stride, droprate=0., activate_before_residual=False):\n        super(Residual3x3Unit, self).__init__()\n        self.bn_0 = BatchNormalization(momentum=0.999)\n        self.relu_0 = LeakyReLU(alpha=0.1)\n        self.conv_0 =Conv2D(channels_out, kernel_size=3, strides=stride, padding='same', use_bias=False)\n        self.bn_1 = BatchNormalization(momentum=0.999)\n        self.relu_1 = LeakyReLU(alpha=0.1)\n        self.conv_1 = Conv2D(channels_out, kernel_size=3, strides=1, padding='same', use_bias=False)\n        self.downsample = channels_in != channels_out\n        self.shortcut = Conv2D(channels_out, kernel_size=1, strides=stride, use_bias=False)\n        self.activate_before_residual = activate_before_residual\n        self.dropout = Dropout(rate=droprate)\n        self.droprate = droprate\n\n    @tf.function\n    def call(self, x, training=True):\n        if self.downsample and self.activate_before_residual:\n            x = self.relu_0(self.bn_0(x, training=training))\n        elif not self.downsample:\n            out = self.relu_0(self.bn_0(x, training=training))\n        out = self.relu_1(self.bn_1(self.conv_0(x if self.downsample else out), training=training))\n        if self.droprate > 0.:\n            out = self.dropout(out)\n        out = self.conv_1(out)\n        return out + (self.shortcut(x) if self.downsample else x)\n\n\nclass ResidualBlock(tf.keras.layers.Layer):\n    def __init__(self, n_units, channels_in, channels_out, unit, stride, droprate=0., activate_before_residual=False):\n        super(ResidualBlock, self).__init__()\n        self.units = self._build_unit(n_units, unit, channels_in, channels_out, stride, droprate, activate_before_residual)\n\n    def _build_unit(self, n_units, unit, channels_in, channels_out, stride, droprate, activate_before_residual):\n        units = []\n        for i in range(n_units):\n            units.append(unit(channels_in if i == 0 else channels_out, \n                        channels_out, stride if i == 0 else 1, droprate, activate_before_residual))\n        return units\n\n    @tf.function\n    def call(self, x, training=True):\n        for unit in self.units:\n            x = unit(x, training=training)\n        return x\n\n\nclass WideResNet(tf.keras.Model):\n    def __init__(self, num_classes, depth=28, width=2, droprate=0., input_shape=(None, 32, 32, 3), **kwargs):\n        super(WideResNet, self).__init__(input_shape, **kwargs)\n        assert (depth - 4) % 6 == 0\n        N = int((depth - 4) / 6)\n        channels = [16, 16 * width, 32 * width, 64 * width]\n\n        self.conv_0 = tf.keras.layers.Conv2D(channels[0], kernel_size=3, strides=1, padding='same', use_bias=False)\n        self.block_0 = ResidualBlock(N, channels[0], channels[1], Residual3x3Unit, 1, droprate, True)\n        self.block_1 = ResidualBlock(N, channels[1], channels[2], Residual3x3Unit, 2, droprate)\n        self.block_2 = ResidualBlock(N, channels[2], channels[3], Residual3x3Unit, 2, droprate)\n        self.bn_0 = BatchNormalization(momentum=0.999)\n        self.relu_0 = LeakyReLU(alpha=0.1)\n        self.avg_pool = AveragePooling2D((8, 8), (1, 1))\n        self.flatten = Flatten()\n        self.dense = Dense(num_classes)\n\n    @tf.function\n    def call(self, inputs, training=True):\n        x = inputs\n        x = self.conv_0(x)\n        x = self.block_0(x, training=training)\n        x = self.block_1(x, training=training)\n        x = self.block_2(x, training=training)\n        x = self.relu_0(self.bn_0(x, training=training))\n        x = self.avg_pool(x)\n        x = self.flatten(x)\n        x = self.dense(x)\n        return x","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def guess_labels(u_aug, model, k):\n    u_logits = tf.nn.softmax(model(u_aug[0]), axis=1)\n    for _ in range(1, k):\n        u_logits = u_logits + tf.nn.softmax(model(u_aug[_]), axis=1)\n    u_logits = u_logits / k\n    u_logits = tf.stop_gradient(u_logits)\n    return u_logits","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@tf.function\ndef sharpen(p, T):\n    return tf.pow(p, 1/T) / tf.reduce_sum(tf.pow(p, 1/T), axis=1, keepdims=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"@tf.function\ndef mixup(x1, x2, y1, y2, beta):\n    beta = tf.maximum(beta, 1-beta)\n    x = beta * x1 + (1 - beta) * x2\n    y = beta * y1 + (1 - beta) * y2\n    return x, y","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def mixmatch(model, X, y, U, T, K, beta):\n    batch_size = X.shape[0]\n    # mean logits from augmentation of unlabelled data.\n    mean_logits = guess_labels(U, model, K)\n    # using the label smoothing technique for sharpening the probability dis.\n    qb = sharpen(mean_logits, T)\n    # repeat the probability dis multiple times(K)\n    qb = tf.concat([qb for _ in range(K)], axis=0)\n    # concatenate both labelled X and unlabelled X and lab_y and unlab_y\n    U = tf.concat([_ for _ in U], axis=0)\n    XU = tf.concat([X, U], axis=0)\n    XUy = tf.concat([y, qb], axis=0)\n    # shuffle the combined dataset.\n    indices = tf.random.shuffle(tf.range(XU.shape[0]))\n    W = tf.gather(XU, indices)\n    Wy = tf.gather(XUy, indices)\n    # and use the mixup data augmentation with the shuffled data and unshuffle data.\n    XU, XUy = mixup(XU, W, XUy, Wy, beta=beta)\n    XU = tf.split(XU, K + 1, axis=0)\n    XU = interleave(XU, batch_size)\n    return XU, XUy\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def interleave_offsets(batch, nu):\n    groups = [batch // (nu + 1)] * (nu + 1)\n    for x in range(batch - sum(groups)):\n        groups[-x - 1] += 1\n    offsets = [0]\n    for g in groups:\n        offsets.append(offsets[-1] + g)\n    assert offsets[-1] == batch\n    return offsets\n\ndef interleave(xy, batch):\n    nu = len(xy) - 1\n    offsets = interleave_offsets(batch, nu)\n    xy = [[v[offsets[p]:offsets[p + 1]] for p in range(nu + 1)] for v in xy]\n    for i in range(1, nu + 1):\n        xy[0][i], xy[i][i] = xy[i][i], xy[0][i]\n    return [tf.concat(v, axis=0) for v in xy]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def ema_weight_update(model, ema_model, ema_decay):\n    ema_vars = ema_model.get_weights()\n    model_vars = model.get_weights()\n    \n    if model_vars:\n        for i in range(len(ema_vars)):\n            ema_vars[i] = (1 - ema_decay) * model_vars[i] + ema_decay * ema_var[i]\n    \n    ema_model.set_weights(ema_vars)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def weight_decay(model, weight_decay):\n    model_vars = model.get_weights()\n    \n    if model_vars:\n        for i in range(len(model_vars)):\n            model_vars[i] = model_vars[i] * (1 - weight_decay)\n    \n    model.set_weights(model_vars)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def semi_loss(labels_x, logits_x, labels_u, logits_u):\n    xe_loss_func = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n    loss_xe = xe_loss_func(labels_x, logits_x)\n   \n    loss_l2u = tf.square(labels_u - tf.nn.softmax(logits_u))\n    loss_l2u = tf.reduce_mean(loss_l2u)\n    return loss_xe, loss_l2u","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_step(labelled_batch, unlabelled_batch, model, ema_model, optimizer,\n                   xe_loss_tracker, l2_loss_tracker, total_loss_tracker, metric_tracker, **kwargs):\n    \n    T = kwargs.get('T')\n    K = kwargs.get('K')\n    beta = kwargs.get('beta')\n    ema_decay_rate = kwargs.get('ema_decay_rate')\n    weight_decay_rate = kwargs.get('weight_decay_rate')\n    lambda_u = kwargs.get('lambda_u')\n    \n\n    train_X, train_y = labelled_batch\n    train_U = unlabelled_batch\n    batch_size = train_X.shape[0]\n    \n    with tf.GradientTape() as tape:\n        # running mixmatch to get a combined training dataset.(unlabeled and labeled)\n        XU, XUy = mixmatch(model, train_X, train_y, train_U, T, K, beta)\n        logits = [model(XU[0])]\n        for batch in XU[1:]:\n            logits.append(model(batch))\n\n        logits = interleave(logits, batch_size)\n        logits_x = logits[0]\n        logits_u = tf.concat(logits[1:], axis=0)\n\n        # compute loss\n        xe_loss, l2u_loss = semi_loss(XUy[: batch_size], logits_x, XUy[batch_size: ], logits_u)\n        total_loss = xe_loss + lambda_u * l2u_loss\n    \n    model_params = model.trainable_weights \n    grads = tape.gradient(total_loss, model_params)\n    optimizer.apply_gradients(zip(grads, model_params))\n    \n    # update the weights of both models.\n    ema_weight_update(model, ema_model, ema_decay_rate)\n    weight_decay(model, weight_decay_rate)\n    \n    metric_obj_func = tf.keras.metrics.CategoricalAccuracy()\n    acc = metric_obj_func(train_y, model(train_X))\n    xe_loss_tracker.update_state(xe_loss)\n    l2_loss_tracker.update_state(l2u_loss)\n    total_loss_tracker.update_state(total_loss)\n    metric_tracker.update_state(acc)\n    \n    return {\n        \"accuracy\": metric_tracker.result(),\n        'xe_loss': xe_loss_tracker.result(),\n        \"l2_loss\": l2_loss_tracker.result(),\n        \"total_loss\": total_loss_tracker.result()\n    }\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def test_step(val_batch, model, metric_tracker, loss_tracker):\n    X, y = val_batch\n    batch_size = X.shape[0]\n    # cal the loss with logits and y\n    logits = model(X, training=False)\n    loss_obj_function = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n    loss_val = loss_obj_function(y, logits)\n    loss_tracker.update_state(loss_val)\n    \n    # cal the acc with logits and y\n    acc_obj_function = tf.keras.metrics.CategoricalAccuracy()\n    acc_val = acc_obj_function(y, logits)\n    metric_tracker.update_state(acc_val)\n    \n    return {\n        'accuracy': metric_tracker.result(),\n        'loss': loss_tracker.result()\n    }","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train(labelled_ds, unlabelled_ds, val_ds, epochs, **kwargs):\n    # loss and metrics trackers\n    xe_loss_tracker = tf.keras.metrics.Mean()\n    l2_loss_tracker = tf.keras.metrics.Mean()\n    total_loss_tracker = tf.keras.metrics.Mean()\n    train_acc_tracker = tf.keras.metrics.Mean()\n    val_loss_tracker = tf.keras.metrics.Mean()\n    val_acc_tracker = tf.keras.metrics.Mean()\n    \n    # arguments\n    K = kwargs.get(\"K\")\n    beta = kwargs.get('beta')\n    T = kwargs.get(\"T\")\n    ema_decay_rate = kwargs.get('ema_decay_rate')\n    weight_decay_rate = kwargs.get('weight_decay_rate')\n    learning_rate = kwargs.get(\"learning_rate\")\n    lambda_u = kwargs.get(\"lambda_u\")\n    n_classes = kwargs.get(\"n_classes\")\n    ckpt_dir = kwargs.get('ckpt_dir')\n    log_path = kwargs.get('log_path')\n    \n    # models and optimizer\n    model = WideResNet(n_classes, depth=28, width=2)\n    ema_model = WideResNet(n_classes, depth=28, width=2)\n    ema_model.set_weights(model.get_weights())\n    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n    \n    # checkpoints\n    model_ckpt = tf.train.Checkpoint(step=tf.Variable(0), optimizer=optimizer, net=model)\n    manager = tf.train.CheckpointManager(model_ckpt, f'{ckpt_dir}/model', max_to_keep=3)\n    # for ema model\n    ema_ckpt = tf.train.Checkpoint(step=tf.Variable(0), net=ema_model)\n    ema_manager = tf.train.CheckpointManager(ema_ckpt, f'{ckpt_dir}/ema', max_to_keep=3)\n    \n    # summary writers\n    train_writer = tf.summary.create_file_writer(f'{log_path}/train')\n    val_writer = tf.summary.create_file_writer(f'{log_path}/validation')\n    \n    model_ckpt.restore(manager.latest_checkpoint)\n    ema_ckpt.restore(ema_manager.latest_checkpoint)\n    if manager.latest_checkpoint:\n        print(\"Restored from {}\".format(manager.latest_checkpoint))\n    else:\n        print(\"Initializing from scratch.\")\n        \n    for epoch in range(epochs):\n        print(f'Epoch; {epoch}')\n        for step, unlabelled_batch in tqdm(enumerate(unlabelled_ds), total=len(unlabelled_ds)):\n            model_ckpt.step.assign_add(1)\n            ema_ckpt.step.assign_add(1)\n            for i, labelled_batch in enumerate(labelled_ds):\n                if i == 1:\n                    break\n                res = train_step(labelled_batch,\n                                 unlabelled_batch,\n                                 model,\n                                 ema_model,\n                                 optimizer,\n                                 xe_loss_tracker,\n                                 l2_loss_tracker, \n                                 total_loss_tracker,\n                                 train_acc_tracker,\n                                 K=K,\n                                 T=T, \n                                 beta=beta,\n                                 ema_decay_rate=ema_decay_rate,\n                                 weight_decay_rate=weight_decay_rate,\n                                 lambda_u=lambda_u\n                            )\n                xe_loss = res.get('xe_loss')\n                l2_loss = res.get('l2_loss')\n                total_loss = res.get('total_loss')\n                accuracy = res.get('accuracy')\n        \n        for val_batch in val_ds:\n            val_res = test_step(val_batch,\n                                model,\n                                val_acc_tracker,\n                                val_loss_tracker\n                                )\n            val_loss = val_res.get(\"loss\")\n            val_accuracy = val_res.get(\"accuracy\")\n            \n        with train_writer.as_default():\n            tf.summary.scalar('xe_loss', xe_loss, step=epoch)\n            tf.summary.scalar('l2u_loss', l2_loss, step=epoch)\n            tf.summary.scalar('total_loss', total_loss, step=epoch)\n            tf.summary.scalar('accuracy', accuracy, step=epoch)\n        \n        with val_writer.as_default():\n            tf.summary.scalar('xe_loss', val_loss, step=epoch)\n            tf.summary.scalar('val_accuracy', val_accuracy, step=epoch)   \n            \n        if epoch % 2 == 0:\n            model_save_path = manager.save(checkpoint_number=int(model_ckpt.step))\n            ema_save_path = ema_manager.save(checkpoint_number=int(ema_ckpt.step))\n            print(f'Saved model checkpoint for epoch {int(model_ckpt.step)} @ {model_save_path}')\n            print(f'Saved ema checkpoint for epoch {int(ema_ckpt.step)} @ {ema_save_path}')\n            \n        print(f\"train_loss: {total_loss}, xe_loss: {xe_loss}, l2_loss: {l2_loss}, train_accuracy: {accuracy}\")\n        print(f\"val_loss: {val_loss}, val_accuracy: {val_accuracy}\")\n    \n    for writer in [train_writer, val_writer]:\n        writer.flush()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train(labelled_ds=labelled_ds,\n      unlabelled_ds=unlabelled_ds,\n      val_ds=val_ds,\n      epochs=20,\n      learning_rate=0.01,\n      n_classes=10,\n      beta=0.75,\n      K=2,\n      T=0.5,\n      ema_weight_decay=0.999,\n      weight_decay_rate=0.02,\n      lambda_u=0.01,\n      log_path='tensorboard_logs',\n      ckpt_dir=\"saved\"\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%load_ext tensorboard\n%tensorboard --logdir tensorboard_logs","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}